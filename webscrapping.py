import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, date
import urllib3
import warnings

# Desactivar advert√®ncies SSL i altres warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore')

def scrape_pitch_cat(data_inici, data_fi):
    url = "http://pitch.cat/noticies/index.php?cat=70"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        with st.spinner('Obtenint dades...'):
            response = requests.get(url, headers=headers, verify=False)
            
            if response.status_code != 200:
                st.error(f"Error en accedir a la web. Codi d'estat: {response.status_code}")
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            dates = []
            titols = []
            descripcions = []
            enllacos = []
            
            noticies = soup.find_all('div', class_='noticia')
            
            for noticia in noticies:
                try:
                    # Buscar la data en l'element h6
                    data_element = noticia.find('h6')
                    if not data_element:
                        continue
                        
                    data_text = data_element.text.strip()
                    # Convertir el format de data de DD.MM.YYYY a DD/MM/YYYY
                    data_parts = data_text.split('.')
                    if len(data_parts) == 3:
                        data_text = f"{data_parts[0]}/{data_parts[1]}/{data_parts[2]}"
                    
                    try:
                        data_noticia = datetime.strptime(data_text, '%d/%m/%Y').date()
                    except ValueError:
                        continue
                    
                    if data_inici <= data_noticia <= data_fi:
                        dates.append(data_text)
                        
                        # Buscar el t√≠tol en l'element h5 > a
                        titol_element = noticia.find('h5').find('a')
                        titols.append(titol_element.text.strip() if titol_element else "Sense t√≠tol")
                        
                        # Buscar la descripci√≥ en el div directe
                        desc_element = noticia.find('div', recursive=False)
                        descripcions.append(desc_element.text.strip() if desc_element else "Sense descripci√≥")
                        
                        # Buscar l'enlla√ß en h5 > a
                        link_element = noticia.find('h5').find('a')
                        if link_element and link_element.get('href'):
                            enllac_complet = f"http://pitch.cat/noticies/{link_element['href']}"
                        else:
                            enllac_complet = "Enlla√ß no disponible"
                        enllacos.append(enllac_complet)
                        
                except Exception as e:
                    continue
            
            if not dates:
                st.info("No s'han trobat not√≠cies en l'interval de dates seleccionat")
                return pd.DataFrame()
                
            df = pd.DataFrame({
                'Data': dates,
                'T√≠tol': titols,
                'Descripci√≥': descripcions,
                'Enlla√ß': enllacos
            })
            
            return df
            
    except Exception as e:
        st.error(f"Error en el scraping: {str(e)}")
        return None

def main():
    st.title("Scraper de Not√≠cies Pitch&Putt")
    
    # Afegir informaci√≥ sobre la seguretat
    st.info("""
    ‚ÑπÔ∏è Nota: Aquesta aplicaci√≥ accedeix a una web HTTP (no segura). 
    Les dades es recullen de manera segura per√≤ el navegador pot mostrar advert√®ncies.
    """)
    
    # Moure els controls de dates a la columna principal
    col1, col2 = st.columns(2)
    
    with col1:
        data_inici = st.date_input(
            "Data d'inici",
            date(2024, 1, 1),
            key="data_inici"
        )
    
    with col2:
        data_fi = st.date_input(
            "Data final",
            date.today(),
            key="data_fi"
        )
    
    # Bot√≥ de cerca al centre
    col_button = st.columns([1, 2, 1])[1]
    with col_button:
        cerca = st.button("üîç Cercar Not√≠cies", use_container_width=True)
    
    # Separador visual
    st.markdown("---")
    
    if cerca:
        if data_inici > data_fi:
            st.error("‚ùå La data d'inici no pot ser posterior a la data final")
        else:
            with st.spinner("Cercant not√≠cies..."):
                df = scrape_pitch_cat(data_inici, data_fi)
                
                if df is not None and not df.empty:
                    # Guardar els resultats a la sessi√≥
                    st.session_state.last_results = df
                    st.session_state.last_dates = (data_inici, data_fi)
                    
                    # Mostrar resum
                    st.success(f"‚úÖ S'han trobat {len(df)} not√≠cies entre {data_inici} i {data_fi}")
                    
                    # Pestanyes per diferents vistes
                    tab1, tab2 = st.tabs(["üìä Taula", "üîó Enlla√ßos"])
                    
                    with tab1:
                        st.dataframe(df, use_container_width=True)
                        
                        # Bot√≥ de desc√†rrega
                        csv = df.to_csv(index=False).encode('utf-8')
                        nom_arxiu = f'noticies_seniors_{data_inici}_{data_fi}.csv'
                        
                        st.download_button(
                            label="üì• Descarregar CSV",
                            data=csv,
                            file_name=nom_arxiu,
                            mime='text/csv',
                            use_container_width=True
                        )
                    
                    with tab2:
                        for index, row in df.iterrows():
                            st.markdown(f"**{row['Data']}**: [{row['T√≠tol']}]({row['Enlla√ß']})")
                
                elif df is not None and df.empty:
                    st.warning("‚ö†Ô∏è No s'han trobat not√≠cies en aquest interval de dates")
    
    # Mostrar els √∫ltims resultats si existeixen
    elif 'last_results' in st.session_state:
        df = st.session_state.last_results
        data_inici, data_fi = st.session_state.last_dates
        
        st.info(f"üìå Mostrant els √∫ltims resultats ({data_inici} - {data_fi})")
        
        tab1, tab2 = st.tabs(["üìä Taula", "üîó Enlla√ßos"])
        
        with tab1:
            st.dataframe(df, use_container_width=True)
            
            csv = df.to_csv(index=False).encode('utf-8')
            nom_arxiu = f'noticies_seniors_{data_inici}_{data_fi}.csv'
            
            st.download_button(
                label="üì• Descarregar CSV",
                data=csv,
                file_name=nom_arxiu,
                mime='text/csv',
                use_container_width=True
            )
        
        with tab2:
            for index, row in df.iterrows():
                st.markdown(f"**{row['Data']}**: [{row['T√≠tol']}]({row['Enlla√ß']})")

if __name__ == "__main__":
    main()
